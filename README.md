# üåê EdgeOpt: Edge-Optimized ONNX Model Converter & Benchmark

**EdgeOpt** is a comprehensive Python toolkit for optimizing PyTorch models for **edge deployment**.  
It streamlines model export, quantization, pruning, benchmarking, and accuracy evaluation ‚Äî all through a single command-line interface.  
Designed for researchers, engineers, and learners alike, EdgeOpt bridges the gap between model training and real-world deployment on resource-constrained devices.

---

## üß≠ Overview

EdgeOpt was built to address a growing challenge in modern AI: **how to run deep learning models efficiently on edge hardware** such as mobile devices, IoT sensors, and embedded boards.  
This toolkit transforms a typical PyTorch model into an optimized, deployable ONNX artifact, evaluated and benchmarked for real-world performance.

It supports:
- ‚úÖ End-to-end model optimization (Export ‚Üí Quantize ‚Üí Prune ‚Üí Benchmark ‚Üí Evaluate)
- ‚öôÔ∏è Dynamic INT8/FP16 quantization
- ‚úÇÔ∏è Structured channel pruning
- üìä Comprehensive benchmarking (latency, throughput, memory)
- üéØ CIFAR-10 accuracy evaluation
- üíª CLI-based workflow for easy automation

---

## üìò Project Achievements and Workflow (2025)

### üéØ Introduction

EdgeOpt was conceived as a comprehensive solution for **deploying and evaluating deep learning models on edge devices**.  
Over the course of its development, multiple modules and workflows were created to optimize neural networks for speed, efficiency, and real-world usability ‚Äî without sacrificing too much accuracy.

---

### üß© Core Achievements

#### 1. End-to-End Optimization Pipeline

At the heart of EdgeOpt is a **unified CLI-driven pipeline** that takes a standard PyTorch model and converts it into a fully optimized ONNX model ‚Äî ready for deployment.

- **Model Export**: Converts common PyTorch architectures to ONNX, ensuring runtime compatibility.  
- **Quantization**: Integrates dynamic INT8/FP16 quantization for low-memory, high-speed inference.  
- **Pruning**: Applies structured channel pruning to shrink model size while retaining accuracy.  
- **Benchmarking**: Measures latency (p50, p95), throughput (FPS), model size, and memory footprint.

These modules together form a reproducible workflow for model optimization on edge platforms like Raspberry Pi, Jetson Nano, or mobile CPUs.

---

#### 2. Automated Accuracy Evaluation

Performance alone is not enough ‚Äî **EdgeOpt quantifies accuracy trade-offs** after each optimization step.

- Runs inference on CIFAR-10 to measure classification accuracy.  
- Compares pre- and post-optimization performance to visualize trade-offs.  
- Provides users with data-driven insight into optimization results.

---

#### 3. Fine-Tuning for Realistic Deployment

Recognizing that pretrained models require adaptation for specific datasets, EdgeOpt provides a **fine-tuning pipeline**:

- Fine-tunes **MobileNetV2** on CIFAR-10 using `scripts/finetune_mobilenet_cifar.py`.  
- Automatically exports, evaluates, and optimizes the trained model.  
- Supports reproducible training and hyperparameter tuning for robust experiments.

---

#### 4. Robust Project Structure & Best Practices

EdgeOpt follows **software engineering best practices** for research and development:

- Clean repository organization and version control.  
- Automated environment setup via `requirements.txt`.  
- Modular, extensible Python codebase for easy customization.  
- Clear documentation and CLI tools for all modules.

---

#### 5. Experimentation & Evaluation

Comprehensive experiments validated every stage of the pipeline:

- Latency and throughput comparison for vanilla, quantized, and pruned models.  
- Accuracy evaluation before and after optimization.  
- Model size reduction analysis for deployment feasibility.  
- Scripts and notebooks for reproducible benchmarking.

---

### üåç Impact and Future Directions

EdgeOpt represents a **holistic approach to edge AI** ‚Äî merging research rigor with engineering practicality.

Future improvements include:
- Support for more architectures (e.g., ResNet, EfficientNet).  
- Hardware-aware profiling on multiple edge platforms.  
- Automated report generation in HTML/Markdown.  
- Collaborative features for open-source contributions.

### üß† Conclusion

EdgeOpt goes beyond model conversion ‚Äî it‚Äôs a **complete edge AI deployment toolkit**.  
It empowers developers to **quantify**, **compare**, and **understand** how optimization affects model performance and accuracy.

> ‚ÄúOptimized models, simplified deployment ‚Äî EdgeOpt brings AI to the edge.‚Äù

---

## ‚öôÔ∏è Quick Start

### Installation

```bash
git clone https://github.com/LasyaRamachandruni/edgeopt.git
cd edgeopt
python -m venv venv
source venv/bin/activate    # On Windows: .\venv\Scripts\activate
pip install -r requirements.txt
pip install -e .
```
üß© Fine-Tuning & Optimization Workflow
1. Fine-tune MobileNetV2 for CIFAR-10
```bash
python scripts/finetune_mobilenet_cifar.py
```

Downloads and prepares CIFAR-10

Adjusts MobileNetV2 classifier for 10 classes

Trains for 5 epochs (adjustable)

Saves model as mobilenetv2_cifar10.pth

2. Export the Trained Model to ONNX
3. Optimize & Evaluate the Model
    ```bash
    edgeopt quantize --input mobilenetv2_cifar10.onnx --output model_int8.onnx --quant-type int8
    edgeopt prune --amount 0.3 --output mobilenetv2_pruned.onnx
    edgeopt benchmark --model model_int8.onnx --N 200 --batch-size 1
    edgeopt evaluate --model model_int8.onnx --batch-size 64
   ```



## üèÅ Experimental Results and Analysis

### üöÄ Workflow Steps

1. **Fine-tune MobileNetV2 on CIFAR-10**
   - Train for several epochs, save as `mobilenetv2_cifar10.pth`
   - Achieved high test accuracy after training

2. **Prune deeper layers and fine-tune for 3 epochs**
   - Loss per epoch:  
     `Epoch 1, Loss: 0.1779`  
     `Epoch 2, Loss: 0.1323`  
     `Epoch 3, Loss: 0.1211`
   - Test accuracy after prune + retrain: **90.35%**
   - Exported to ONNX: `mobilenetv2_pruned_finetuned.onnx`

3. **Dynamic INT8 Quantization of pruned+finetuned ONNX**
   - Output: `mobilenetv2_pruned_finetuned_int8.onnx`

4. **Benchmark and Evaluate All Models**
   - Used CLI to measure latency, throughput, and size

### üìä Results Table

| Model                       | Size (MB) | Latency p50 (ms) | Latency p95 (ms) | Throughput (FPS) | Accuracy (%) |
|-----------------------------|-----------|------------------|------------------|------------------|-------------|
| Pruned + Fine-Tuned ONNX    |   8.51    |      3.75        |     15.95        |     266.47       |   90.35     |
| Pruned + FT + Quantized     |   2.31    |     31.78        |     47.33        |     31.47        |   87.74     |

> *Baseline ONNX results omitted due to lack of reproducible accuracy; focus placed on successful pruning + fine-tuning and quantization pipeline.*

### üí° Discussion

- Pruning with retraining preserved almost all accuracy and provided significant model compression.
- Quantization further reduced model size and memory footprint, enabling real edge deployment.
- The CLI workflow is fully reproducible; results can be replicated and extended for additional architectures.



üß± Module Summary
File	Description
export.py	Converts PyTorch models to ONNX format with verification
quantize.py	Performs dynamic quantization (INT8/FP16)
prune.py	Structured channel pruning for model compression
benchmark.py	Reports latency, throughput, memory, and size metrics
eval.py	Evaluates ONNX models on CIFAR-10 for accuracy
cli.py	Provides a unified command-line interface for all modules

üßæ Requirements
Python 3.8+
PyTorch 2.0+
ONNX Runtime
NumPy
torchvision
tqdm
psutil
Full list available in requirements.txt.

üôè Acknowledgments
Built using:
PyTorch
ONNX Runtime

Developed as part of AI/ML research at San Jose State University, with inspiration from real-world edge AI optimization challenges.

üìö Citation

If you use EdgeOpt in your research or project, please cite:

@software{edgeopt2025,
  title = {EdgeOpt: Edge-Optimized ONNX Model Converter & Benchmark},
  author = {Ramachandruni, Swathi Sri Lasya Mayukha},
  year = {2025},
  url = {https://github.com/LasyaRamachandruni/edgeopt}
}

üì´ Contact

Author: Swathi Sri Lasya Mayukha Ramachandruni
Email: swathisrilasyamayukha.ramachandruni@sjsu.edu
GitHub: @LasyaRamachandruni

‚ú® EdgeOpt is where deep learning meets deployment ‚Äî optimized, efficient, and ready for the edge.
